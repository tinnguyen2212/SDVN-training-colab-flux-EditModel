{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8KQYvGDO7cg"
   },
   "outputs": [],
   "source": [
    "# @title Script\n",
    "!pip install huggingface-sb3\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import HfApi\n",
    "read_token = \"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\"\n",
    "login(read_token, add_to_git_credential=True)\n",
    "api = HfApi()\n",
    "user = api.whoami(read_token)\n",
    "\n",
    "import requests, copy, os, torch, gc\n",
    "from torch import nn\n",
    "from transformers import  AutoModel, AutoProcessor, AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast, AutoModelForCausalLM\n",
    "from pathlib import Path\n",
    "import torch.amp.autocast_mode\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TVF\n",
    "from unittest.mock import patch\n",
    "from IPython.display import clear_output,display, HTML\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "from transformers.dynamic_module_utils import get_imports\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from PIL import Image\n",
    "import io, base64, json, yaml\n",
    "\n",
    "gpu_name = torch.cuda.get_device_name()\n",
    "print(gpu_name)\n",
    "if 'A100' in gpu_name:\n",
    "  os.environ['TORCH_CUDA_ARCH_LIST'] = '8.0'\n",
    "if 'L4' in gpu_name:\n",
    "  os.environ['TORCH_CUDA_ARCH_LIST'] = '8.9'\n",
    "if 'T4' in gpu_name:\n",
    "  os.environ['TORCH_CUDA_ARCH_LIST'] = '7.5'\n",
    "\n",
    "CLIP_PATH = \"google/siglip-so400m-patch14-384\"\n",
    "CHECKPOINT_PATH = Path(\"/content/joy-caption-alpha-two/cgrkzexw-599808\")\n",
    "TITLE = \"<h1><center>JoyCaption Alpha Two (2024-09-26a)</center></h1>\"\n",
    "CAPTION_TYPE_MAP = {\n",
    "\t\"Descriptive\": [\n",
    "\t\t\"Write a descriptive caption for this image in a formal tone.\",\n",
    "\t\t\"Write a descriptive caption for this image in a formal tone within {word_count} words.\",\n",
    "\t\t\"Write a {length} descriptive caption for this image in a formal tone.\",\n",
    "\t],\n",
    "\t\"Descriptive (Informal)\": [\n",
    "\t\t\"Write a descriptive caption for this image in a casual tone.\",\n",
    "\t\t\"Write a descriptive caption for this image in a casual tone within {word_count} words.\",\n",
    "\t\t\"Write a {length} descriptive caption for this image in a casual tone.\",\n",
    "\t],\n",
    "\t\"Training Prompt\": [\n",
    "\t\t\"Write a stable diffusion prompt for this image.\",\n",
    "\t\t\"Write a stable diffusion prompt for this image within {word_count} words.\",\n",
    "\t\t\"Write a {length} stable diffusion prompt for this image.\",\n",
    "\t],\n",
    "\t\"MidJourney\": [\n",
    "\t\t\"Write a MidJourney prompt for this image.\",\n",
    "\t\t\"Write a MidJourney prompt for this image within {word_count} words.\",\n",
    "\t\t\"Write a {length} MidJourney prompt for this image.\",\n",
    "\t],\n",
    "\t\"Booru tag list\": [\n",
    "\t\t\"Write a list of Booru tags for this image.\",\n",
    "\t\t\"Write a list of Booru tags for this image within {word_count} words.\",\n",
    "\t\t\"Write a {length} list of Booru tags for this image.\",\n",
    "\t],\n",
    "\t\"Booru-like tag list\": [\n",
    "\t\t\"Write a list of Booru-like tags for this image.\",\n",
    "\t\t\"Write a list of Booru-like tags for this image within {word_count} words.\",\n",
    "\t\t\"Write a {length} list of Booru-like tags for this image.\",\n",
    "\t],\n",
    "\t\"Art Critic\": [\n",
    "\t\t\"Analyze this image like an art critic would with information about its composition, style, symbolism, the use of color, light, any artistic movement it might belong to, etc.\",\n",
    "\t\t\"Analyze this image like an art critic would with information about its composition, style, symbolism, the use of color, light, any artistic movement it might belong to, etc. Keep it within {word_count} words.\",\n",
    "\t\t\"Analyze this image like an art critic would with information about its composition, style, symbolism, the use of color, light, any artistic movement it might belong to, etc. Keep it {length}.\",\n",
    "\t],\n",
    "\t\"Product Listing\": [\n",
    "\t\t\"Write a caption for this image as though it were a product listing.\",\n",
    "\t\t\"Write a caption for this image as though it were a product listing. Keep it under {word_count} words.\",\n",
    "\t\t\"Write a {length} caption for this image as though it were a product listing.\",\n",
    "\t],\n",
    "\t\"Social Media Post\": [\n",
    "\t\t\"Write a caption for this image as if it were being used for a social media post.\",\n",
    "\t\t\"Write a caption for this image as if it were being used for a social media post. Limit the caption to {word_count} words.\",\n",
    "\t\t\"Write a {length} caption for this image as if it were being used for a social media post.\",\n",
    "\t],\n",
    "}\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
    "\n",
    "class ImageAdapter(nn.Module):\n",
    "\tdef __init__(self, input_features: int, output_features: int, ln1: bool, pos_emb: bool, num_image_tokens: int, deep_extract: bool):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.deep_extract = deep_extract\n",
    "\n",
    "\t\tif self.deep_extract:\n",
    "\t\t\tinput_features = input_features * 5\n",
    "\n",
    "\t\tself.linear1 = nn.Linear(input_features, output_features)\n",
    "\t\tself.activation = nn.GELU()\n",
    "\t\tself.linear2 = nn.Linear(output_features, output_features)\n",
    "\t\tself.ln1 = nn.Identity() if not ln1 else nn.LayerNorm(input_features)\n",
    "\t\tself.pos_emb = None if not pos_emb else nn.Parameter(torch.zeros(num_image_tokens, input_features))\n",
    "\n",
    "\t\t# Other tokens (<|image_start|>, <|image_end|>, <|eot_id|>)\n",
    "\t\tself.other_tokens = nn.Embedding(3, output_features)\n",
    "\t\tself.other_tokens.weight.data.normal_(mean=0.0, std=0.02)   # Matches HF's implementation of llama3\n",
    "\n",
    "\tdef forward(self, vision_outputs: torch.Tensor):\n",
    "\t\tif self.deep_extract:\n",
    "\t\t\tx = torch.concat((\n",
    "\t\t\t\tvision_outputs[-2],\n",
    "\t\t\t\tvision_outputs[3],\n",
    "\t\t\t\tvision_outputs[7],\n",
    "\t\t\t\tvision_outputs[13],\n",
    "\t\t\t\tvision_outputs[20],\n",
    "\t\t\t), dim=-1)\n",
    "\t\t\tassert len(x.shape) == 3, f\"Expected 3, got {len(x.shape)}\"  # batch, tokens, features\n",
    "\t\t\tassert x.shape[-1] == vision_outputs[-2].shape[-1] * 5, f\"Expected {vision_outputs[-2].shape[-1] * 5}, got {x.shape[-1]}\"\n",
    "\t\telse:\n",
    "\t\t\tx = vision_outputs[-2]\n",
    "\n",
    "\t\tx = self.ln1(x)\n",
    "\n",
    "\t\tif self.pos_emb is not None:\n",
    "\t\t\tassert x.shape[-2:] == self.pos_emb.shape, f\"Expected {self.pos_emb.shape}, got {x.shape[-2:]}\"\n",
    "\t\t\tx = x + self.pos_emb\n",
    "\n",
    "\t\tx = self.linear1(x)\n",
    "\t\tx = self.activation(x)\n",
    "\t\tx = self.linear2(x)\n",
    "\n",
    "\t\t# <|image_start|>, IMAGE, <|image_end|>\n",
    "\t\tother_tokens = self.other_tokens(torch.tensor([0, 1], device=self.other_tokens.weight.device).expand(x.shape[0], -1))\n",
    "\t\tassert other_tokens.shape == (x.shape[0], 2, x.shape[2]), f\"Expected {(x.shape[0], 2, x.shape[2])}, got {other_tokens.shape}\"\n",
    "\t\tx = torch.cat((other_tokens[:, 0:1], x, other_tokens[:, 1:2]), dim=1)\n",
    "\n",
    "\t\treturn x\n",
    "\n",
    "\tdef get_eot_embedding(self):\n",
    "\t\treturn self.other_tokens(torch.tensor([2], device=self.other_tokens.weight.device)).squeeze(0)\n",
    "\n",
    "def delete_specific_models():\n",
    "    variable_names = ['clip_model', 'tokenizer', 'text_model', 'image_adapter']\n",
    "    for var_name in variable_names:\n",
    "        if var_name in globals():\n",
    "            del globals()[var_name]\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "#API\n",
    "model_list = {\n",
    "    \"APIGemini | 2.0 Flash\" : \"gemini-2.0-flash-001\",\n",
    "    \"APIGemini | 2.0 Flash Lite\": \"gemini-2.0-flash-lite-preview-02-05\",\n",
    "    \"APIOpenAI | GPT 4-o mini\": \"gpt-4o-mini\",\n",
    "}\n",
    "\n",
    "def encode_image(image):\n",
    "    with io.BytesIO() as image_buffer:\n",
    "        image.save(image_buffer, format=\"PNG\")\n",
    "        image_buffer.seek(0)\n",
    "        encoded_image = base64.b64encode(image_buffer.read()).decode('utf-8')\n",
    "    return encoded_image\n",
    "\n",
    "def api_check():\n",
    "    api_file = os.path.join(data_dir,\"Setting/API_key_for_sdvn_comfy_node.json\")\n",
    "    if os.path.exists(api_file):\n",
    "        with open(api_file, 'r', encoding='utf-8') as f:\n",
    "            api_list = json.load(f)\n",
    "        return api_list\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def api_caption(image, length:int, APIkey, Caption, prompt):\n",
    "    if APIkey == \"\":\n",
    "        api_list = api_check()\n",
    "        if api_check() != None:\n",
    "            if \"Gemini\" in Caption:\n",
    "                APIkey =  api_list[\"Gemini\"]\n",
    "            if \"OpenAI\" in Caption:\n",
    "                APIkey =  api_list[\"OpenAI\"]\n",
    "    model_name = model_list[Caption]\n",
    "    prompt += f\"Picture description, Send the description on demand, limit {length} words, only send me the answer, Always return English. \"\n",
    "    if 'Gemini' in Caption:\n",
    "        client = genai.Client(api_key=APIkey)\n",
    "        response = client.models.generate_content(\n",
    "                    model=model_name,\n",
    "                    contents=[prompt, image])\n",
    "        answer = response.text\n",
    "    if \"OpenAI\" in Caption:\n",
    "        answer = \"\"\n",
    "        client = OpenAI(\n",
    "            api_key=APIkey)\n",
    "        if image != None:\n",
    "            image = encode_image(image)\n",
    "            prompt = [{\"type\": \"text\", \"text\": prompt, }, {\n",
    "                \"type\": \"image_url\", \"image_url\": {\"url\":  f\"data:image/jpeg;base64,{image}\"}, },]\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt }]\n",
    "        stream = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                answer += chunk.choices[0].delta.content\n",
    "        if image != None:\n",
    "            answer = answer.split('return True')[-1]\n",
    "    return answer.strip()\n",
    "\n",
    "@torch.no_grad()\n",
    "def joy_caption(input_image: Image.Image, caption_type: str, caption_length: str | int, extra_options: list[str], name_input: str, custom_prompt: str) -> tuple[str, str]:\n",
    "\ttorch.cuda.empty_cache()\n",
    "\n",
    "\t# 'any' means no length specified\n",
    "\tlength = None if caption_length == \"any\" else caption_length\n",
    "\n",
    "\tif isinstance(length, str):\n",
    "\t\ttry:\n",
    "\t\t\tlength = int(length)\n",
    "\t\texcept ValueError:\n",
    "\t\t\tpass\n",
    "\t\n",
    "\t# Build prompt\n",
    "\tif length is None:\n",
    "\t\tmap_idx = 0\n",
    "\telif isinstance(length, int):\n",
    "\t\tmap_idx = 1\n",
    "\telif isinstance(length, str):\n",
    "\t\tmap_idx = 2\n",
    "\telse:\n",
    "\t\traise ValueError(f\"Invalid caption length: {length}\")\n",
    "\t\n",
    "\tprompt_str = CAPTION_TYPE_MAP[caption_type][map_idx]\n",
    "\n",
    "\t# Add extra options\n",
    "\tif len(extra_options) > 0:\n",
    "\t\tprompt_str += \" \" + \" \".join(extra_options)\n",
    "\t\n",
    "\t# Add name, length, word_count\n",
    "\tprompt_str = prompt_str.format(name=name_input, length=caption_length, word_count=caption_length)\n",
    "\n",
    "\tif custom_prompt.strip() != \"\":\n",
    "\t\tprompt_str = custom_prompt.strip()\n",
    "\n",
    "\t# Preprocess image\n",
    "\t# NOTE: I found the default processor for so400M to have worse results than just using PIL directly\n",
    "\t#image = clip_processor(images=input_image, return_tensors='pt').pixel_values\n",
    "\timage = input_image.resize((384, 384), Image.LANCZOS)\n",
    "\tpixel_values = TVF.pil_to_tensor(image).unsqueeze(0) / 255.0\n",
    "\tpixel_values = TVF.normalize(pixel_values, [0.5], [0.5])\n",
    "\tpixel_values = pixel_values.to('cuda')\n",
    "\n",
    "\t# Embed image\n",
    "\t# This results in Batch x Image Tokens x Features\n",
    "\twith torch.amp.autocast_mode.autocast('cuda', enabled=True):\n",
    "\t\tvision_outputs = clip_model(pixel_values=pixel_values, output_hidden_states=True)\n",
    "\t\tembedded_images = image_adapter(vision_outputs.hidden_states)\n",
    "\t\tembedded_images = embedded_images.to('cuda')\n",
    "\t\n",
    "\t# Build the conversation\n",
    "\tconvo = [\n",
    "\t\t{\n",
    "\t\t\t\"role\": \"system\",\n",
    "\t\t\t\"content\": \"You are a helpful image captioner.\",\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"role\": \"user\",\n",
    "\t\t\t\"content\": prompt_str,\n",
    "\t\t},\n",
    "\t]\n",
    "\n",
    "\t# Format the conversation\n",
    "\tconvo_string = tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = True)\n",
    "\tassert isinstance(convo_string, str)\n",
    "\n",
    "\t# Tokenize the conversation\n",
    "\t# prompt_str is tokenized separately so we can do the calculations below\n",
    "\tconvo_tokens = tokenizer.encode(convo_string, return_tensors=\"pt\", add_special_tokens=False, truncation=False)\n",
    "\tprompt_tokens = tokenizer.encode(prompt_str, return_tensors=\"pt\", add_special_tokens=False, truncation=False)\n",
    "\tassert isinstance(convo_tokens, torch.Tensor) and isinstance(prompt_tokens, torch.Tensor)\n",
    "\tconvo_tokens = convo_tokens.squeeze(0)   # Squeeze just to make the following easier\n",
    "\tprompt_tokens = prompt_tokens.squeeze(0)\n",
    "\n",
    "\t# Calculate where to inject the image\n",
    "\teot_id_indices = (convo_tokens == tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")).nonzero(as_tuple=True)[0].tolist()\n",
    "\tassert len(eot_id_indices) == 2, f\"Expected 2 <|eot_id|> tokens, got {len(eot_id_indices)}\"\n",
    "\n",
    "\tpreamble_len = eot_id_indices[1] - prompt_tokens.shape[0]   # Number of tokens before the prompt\n",
    "\n",
    "\t# Embed the tokens\n",
    "\tconvo_embeds = text_model.model.embed_tokens(convo_tokens.unsqueeze(0).to('cuda'))\n",
    "\n",
    "\t# Construct the input\n",
    "\tinput_embeds = torch.cat([\n",
    "\t\tconvo_embeds[:, :preamble_len],   # Part before the prompt\n",
    "\t\tembedded_images.to(dtype=convo_embeds.dtype),   # Image\n",
    "\t\tconvo_embeds[:, preamble_len:],   # The prompt and anything after it\n",
    "\t], dim=1).to('cuda')\n",
    "\n",
    "\tinput_ids = torch.cat([\n",
    "\t\tconvo_tokens[:preamble_len].unsqueeze(0),\n",
    "\t\ttorch.zeros((1, embedded_images.shape[1]), dtype=torch.long),   # Dummy tokens for the image (TODO: Should probably use a special token here so as not to confuse any generation algorithms that might be inspecting the input)\n",
    "\t\tconvo_tokens[preamble_len:].unsqueeze(0),\n",
    "\t], dim=1).to('cuda')\n",
    "\tattention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "\tgenerate_ids = text_model.generate(input_ids, inputs_embeds=input_embeds, attention_mask=attention_mask, max_new_tokens=300, do_sample=True, suppress_tokens=None)   # Uses the default which is temp=0.6, top_p=0.9\n",
    "\n",
    "\t# Trim off the prompt\n",
    "\tgenerate_ids = generate_ids[:, input_ids.shape[1]:]\n",
    "\tif generate_ids[0][-1] == tokenizer.eos_token_id or generate_ids[0][-1] == tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"):\n",
    "\t\tgenerate_ids = generate_ids[:, :-1]\n",
    "\n",
    "\tcaption = tokenizer.batch_decode(generate_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "\treturn caption.strip().replace(\"\\n\", \"\")\n",
    " \n",
    "version = \"large\"\n",
    "device = torch.device(torch.cuda.current_device())\n",
    "\n",
    "def clean_directory(directory):\n",
    "  supported_types = [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".safetensors\"]\n",
    "  for item in os.listdir(directory):\n",
    "      file_path = os.path.join(directory, item)\n",
    "      if os.path.isfile(file_path):\n",
    "          file_ext = os.path.splitext(item)[1]\n",
    "          if file_ext not in supported_types:\n",
    "              print(f\"Deleting file {item} from {directory}\")\n",
    "              os.remove(file_path)\n",
    "      elif os.path.isdir(file_path):\n",
    "          clean_directory(file_path)\n",
    "\n",
    "def fixed_get_imports(filename: str | os.PathLike) -> list[str]:\n",
    "    \"\"\"Workaround for FlashAttention\"\"\"\n",
    "    if os.path.basename(filename) != \"modeling_florence2.py\":\n",
    "        return get_imports(filename)\n",
    "    imports = get_imports(filename)\n",
    "    # imports.remove(\"flash_attn\")\n",
    "    return imports\n",
    "\n",
    "def load_model(version, device):\n",
    "    model_dir = \"/content/Model\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    \n",
model_path = "/content/MODELS/FLUX.1"  # Đường dẫn đến model FLUX.1

model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)
processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
\n",
    "        processor = AutoProcessor.from_pretrained(identifier, cache_dir=model_dir, trust_remote_code=True)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    return (model, processor)\n",
    "\n",
    "def load(version, device):\n",
    "  if 'processor' not in globals():\n",
    "    global model, processor\n",
    "    model, processor = load_model(version, device)\n",
    "    \n",
    "def run_example(task_prompt, image, max_new_tokens, num_beams, do_sample, text_input=None):\n",
    "    if text_input is None:\n",
    "        prompt = task_prompt\n",
    "    else:\n",
    "        prompt = task_prompt + text_input\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        early_stopping=False,\n",
    "        do_sample=do_sample,\n",
    "        num_beams=num_beams,\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    parsed_answer = processor.post_process_generation(\n",
    "        generated_text,\n",
    "        task=task_prompt,\n",
    "        image_size=(image.width, image.height)\n",
    "    )\n",
    "    return parsed_answer\n",
    "\n",
    "def florence_caption(task_prompt, image, max_new_tokens = 1024, num_beams = 3, do_sample = False, fill_mask = False, text_input=None):\n",
    "    if task_prompt == '<CAPTION>':\n",
    "        result = run_example(task_prompt, image, max_new_tokens, num_beams, do_sample)\n",
    "        return result[task_prompt].replace(\"\\n\", \"\")\n",
    "    elif task_prompt == '<DETAILED_CAPTION>':\n",
    "        result = run_example(task_prompt, image, max_new_tokens, num_beams, do_sample)\n",
    "        return result[task_prompt].replace(\"\\n\", \"\")\n",
    "    elif task_prompt == '<MORE_DETAILED_CAPTION>':\n",
    "        task_prompt = '<MORE_DETAILED_CAPTION>'\n",
    "        result = run_example(task_prompt, image, max_new_tokens, num_beams, do_sample)\n",
    "        return result[task_prompt].replace(\"\\n\", \"\")\n",
    "            \n",
    "def Blip_caption(min_length,max_length,image):\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, min_length=min_length, max_length=max_length)\n",
    "    generated_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "def caption_dir(image_dir,prompt):\n",
    "  if Caption == 'Florence':\n",
    "    load(version, device)\n",
    "  for img_file in os.listdir(image_dir):\n",
    "      file_path = os.path.join(image_dir, img_file)\n",
    "      if os.path.isdir(file_path) :\n",
    "          caption_dir(file_path,prompt)\n",
    "      if img_file.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".PNG\", \".JPG\", \".JPEG\")):\n",
    "          img_path = os.path.join(image_dir, img_file)\n",
    "          image = Image.open(img_path).convert(\"RGB\")\n",
    "          if Caption == 'Blip':\n",
    "            cap = Blip_caption(Cap_prompt[Caption_Length][1],Cap_prompt[Caption_Length][2],image)\n",
    "          elif Caption == 'Florence':\n",
    "            cap = florence_caption(prompt,image).replace('The image shows','')\n",
    "          elif Caption == 'Joy_Caption':\n",
    "            cap = joy_caption(image, Joy_Type, Cap_prompt[Caption_Length][3], [Joy_Extra_Option], Joy_Character_Name, Joy_Custom_Prompt)\n",
    "          else:\n",
    "            cap = api_caption(image, Cap_prompt[Caption_Length][3], APIkey, Caption, API_Prompt)\n",
    "          txt_path = os.path.join(image_dir, f\"{os.path.splitext(img_file)[0]}{extension}\")\n",
    "          with open(txt_path, \"w\") as f:\n",
    "              f.write(cap)\n",
    "          print(f\"Miêu tả của ảnh {img_file}: {cap}\")  \n",
    "  torch.cuda.empty_cache()\n",
    "def read_file(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        contents = f.read()\n",
    "    return contents\n",
    "\n",
    "def write_file(filename, contents):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(contents)\n",
    "\n",
    "def process_tags(filename, custom_tag, append, remove_tag):\n",
    "    contents = read_file(filename)\n",
    "    if remove_tag:\n",
    "      contents = contents.replace(custom_tag, \"\")\n",
    "    else:\n",
    "      tags = [tag.strip() for tag in contents.split(',')]\n",
    "      custom_tags = [tag.strip() for tag in custom_tag.split(',')]\n",
    "      for custom_tag in custom_tags:\n",
    "          custom_tag = custom_tag.replace(\"_\", \" \")\n",
    "          if custom_tag not in tags:\n",
    "              if append:\n",
    "                  tags.append(custom_tag)\n",
    "              else:\n",
    "                  tags.insert(0, custom_tag)\n",
    "      contents = ', '.join(tags)\n",
    "    write_file(filename, contents)\n",
    "\n",
    "def check_dir(image_dir):\n",
    "  if not any([filename.endswith(extension) for filename in os.listdir(image_dir)]):\n",
    "      for filename in os.listdir(image_dir):\n",
    "          if filename.endswith(((\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".PNG\", \".JPG\", \".JPEG\"))):\n",
    "              open(\n",
    "                  os.path.join(image_dir, filename.split(\".\")[0] + extension),\n",
    "                  \"w\",\n",
    "              ).close()\n",
    "\n",
    "def process_dir(image_dir, tag, append, remove_tag):\n",
    "  check_dir(image_dir)\n",
    "  for filename in os.listdir(image_dir):\n",
    "      file_path = os.path.join(image_dir, filename)\n",
    "      if os.path.isdir(file_path) :\n",
    "          print(filename)\n",
    "          process_dir(file_path, tag, append, remove_tag)\n",
    "      elif filename.endswith(extension):\n",
    "          process_tags(file_path, tag, append, remove_tag)\n",
    "\n",
    "def add_forder_name(folder):\n",
    "  for filename in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    if os.path.isdir(file_path):\n",
    "      folder_name = os.path.basename(file_path)\n",
    "      try:\n",
    "          steps, name = folder_name.split('_', 1)\n",
    "          steps = int(steps)\n",
    "      except ValueError:\n",
    "          name = folder_name\n",
    "      name = name.replace(\"/\", \", \")\n",
    "      process_dir(file_path, name, False, False)\n",
    "      add_forder_name(file_path)\n",
    "      \n",
    "def get_steps(folder):\n",
    "    folder_name = os.path.basename(folder)\n",
    "    try:\n",
    "        steps, name = folder_name.split('_', 1)\n",
    "        steps = int(steps)\n",
    "    except ValueError:\n",
    "        steps = Steps\n",
    "        name = folder_name\n",
    "    return steps, name\n",
    "\n",
    "def random_sample(folder):\n",
    "  import random\n",
    "  txt_files = [f for f in os.listdir(folder) if f.endswith('.txt')]\n",
    "  try:\n",
    "    sample = read_file(f\"{folder}/{random.choice(txt_files)}\")\n",
    "    sample = sample.replace('\"', r'\\\"')\n",
    "    sample = f\"[trigger], {sample}\"\n",
    "  except IndexError:\n",
    "    sample = \"[trigger], girl portrait, smile\"\n",
    "  return sample\n",
    "\n",
    "def get_supported_images(folder):\n",
    "  import glob\n",
    "  supported_extensions = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".PNG\", \".JPG\", \".JPEG\")\n",
    "  list_img = [file for ext in supported_extensions for file in glob.glob(f\"{folder}/*{ext}\")]\n",
    "  for img_file in os.listdir(folder):\n",
    "      file_path = os.path.join(folder, img_file)\n",
    "      if os.path.isdir(file_path) :\n",
    "          list_img = list_img + get_supported_images(file_path)\n",
    "  return list_img\n",
    "\n",
    "def replace(old_string, new_string):\n",
    "    import re\n",
    "    with open(file_path, 'r') as file:\n",
    "        yaml_content = file.read()\n",
    "    updated_content = re.sub(old_string, new_string, yaml_content)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(updated_content)\n",
    "\n",
    "default_value = {\n",
    "    \"config_dir\" : \"/content/SDVN-training-colab-flux/Config\",\n",
    "    \"Lora_name\" : \"lora_name\",\n",
    "    \"folder_train\": \"/content/drive/MyDrive/SD-Data\",\n",
    "    \"OutputFolder\": \"/content/drive/MyDrive/SD-Data/Lora\",\n",
    "    \"Custom_Caption\": \"\",\n",
    "    \"Dim\": 32,\n",
    "    \"Alpha\": 16,\n",
    "    \"Save_steps\": 1000,\n",
    "    \"Resolution\": \"512,768,1024\",\n",
    "    \"Batch_size\": 4,\n",
    "    \"Train_TexEncoder\": False,\n",
    "    \"Lr\": 1e-4,\n",
    "    \"Sampler_Steps\": 100,\n",
    "    \"Low_VRAM\": True,\n",
    "    \"Sampler_Prompt\": \"\",\n",
    "}\n",
    "def check_value(default_value):\n",
    "    for key, value in default_value.items():\n",
    "        if key not in globals():\n",
    "            globals()[key] = value\n",
    "        elif type(globals()[key]) == str:\n",
    "            globals()[key] = globals()[key].split(' ')[-1]\n",
    "\n",
    "def config(folder):\n",
    "  check_value(default_value)\n",
    "  with open(f'{config_dir}/config.yaml', 'r') as file:\n",
    "    config_dict = yaml.safe_load(file)\n",
    "  file_path = f'{config_dir}/config_{folder[\"name\"]}.yaml'\n",
    "\n",
    "  config_dict[\"config\"][\"process\"][0][\"sample\"][\"prompts\"]\n",
    "\n",
    "  config_dict[\"config\"][\"name\"] = Lora_name if len(folder_train) == 1 else folder[\"name\"]\n",
    "\n",
    "  config_dict[\"config\"][\"process\"][0][\"training_folder\"] = OutputFolder\n",
    "  config_dict[\"config\"][\"process\"][0][\"trigger_word\"] = Custom_Caption if Custom_Caption != \"\" else folder[\"name\"]\n",
    "\n",
    "  config_dict[\"config\"][\"process\"][0][\"network\"][\"linear\"] = Dim\n",
    "  config_dict[\"config\"][\"process\"][0][\"network\"][\"linear_alpha\"] = Alpha\n",
    "  config_dict[\"config\"][\"process\"][0][\"save\"][\"save_every\"] = Save_steps\n",
    "\n",
    "  config_dict[\"config\"][\"process\"][0][\"datasets\"][0][\"folder_path\"] = folder[\"path\"]\n",
    "  config_dict[\"config\"][\"process\"][0][\"datasets\"][0][\"resolution\"] = [int(x) for x in Resolution.split(',')]\n",
    "\n",
    "  config_dict[\"config\"][\"process\"][0][\"train\"][\"batch_size\"] = Batch_size\n",
    "  config_dict[\"config\"][\"process\"][0][\"train\"][\"steps\"] = folder[\"steps\"]\n",
    "  config_dict[\"config\"][\"process\"][0][\"train\"][\"train_text_encoder\"] = Train_TexEncoder\n",
    "  config_dict[\"config\"][\"process\"][0][\"train\"][\"lr\"] = Lr\n",
    "  config_dict[\"config\"][\"process\"][0][\"train\"][\"disable_sampling\"] = False if Sampler_Steps > 0 else True\n",
    "  config_dict[\"config\"][\"process\"][0][\"model\"][\"low_vram\"] = Low_VRAM\n",
    "\n",
    "  config_dict[\"config\"][\"process\"][0][\"sample\"][\"sample_every\"] = Sampler_Steps if Sampler_Steps > 0 else 100\n",
    "  config_dict[\"config\"][\"process\"][0][\"sample\"][\"prompts\"][0] = random_sample(folder[\"path\"]) if Sampler_Prompt == \"\" else Sampler_Prompt\n",
    "\n",
    "  config_dict[\"meta\"][\"name\"] = Lora_name if len(folder_train) == 1 else folder[\"name\"]\n",
    "  config_dict[\"meta\"][\"version\"] = \"Train by trainlora.vn | stablediffusion.vn\"\n",
    "  with open(file_path, 'w') as file:\n",
    "      yaml.dump(config_dict, file, default_flow_style=False)\n",
    "\n",
    "  print('=====================')\n",
    "  print(f'Thư mục train: {folder[\"path\"]}')\n",
    "  print(f'  Số lượng ảnh: {len(get_supported_images(folder[\"path\"]))}')\n",
    "  print(f'  Số Steps: {folder[\"steps\"]}')\n",
    "  print(f'  Output: {OutputFolder}/{folder[\"name\"]}')\n",
    "  print('=====================')\n",
    "\n",
    "def train():\n",
    "  %cd {toolkit_dir}\n",
    "  delete_specific_models()\n",
    "  for folder in folder_train:\n",
    "    !python run.py {config_dir}/config_{folder[\"name\"]}.yaml\n",
    "\n",
    "clear_output()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
